{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankajarm/cohere_sample_use_case/blob/main/notebooks/BBC_Article_Recommender_with_Text_Embedding_Classification_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0FyHWcc6RI9"
      },
      "source": [
        "## Article Recommender with Text Embedding, Classification, and Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGX0RfPUfFgq"
      },
      "source": [
        "This is a simple demonstration of how we can stack multiple NLP models together \n",
        "to get an output much closer to our desired outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkGvDUYd2gnh"
      },
      "source": [
        "Embeddings can capture the meaning of a piece of text beyond keyword-matching. In this article, we will build a simple news article recommender system that computes the embeddings of all available articles and recommend the most relevant articles based on embeddings similarity. \n",
        "\n",
        "We will also make the recommendation tighter by using text classification to recommend only articles within the same category. We will then extract a list of tags from each recommended article, which can further help readers discover new articles. \n",
        "\n",
        "All this will be done via three Cohere API endpoints stacked together: Embed, Classify, and Generate.\n",
        "\n",
        "![Article recommender with Embed, Classify, and Generate](https://github.com/cohere-ai/notebooks/raw/main/notebooks/images/article-recommender/article-rec-1.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNvRai38Isqr"
      },
      "source": [
        "We will implement the following steps:\n",
        "\n",
        "**1: Find the most similar articles to the one currently reading using embeddings.**\n",
        "\n",
        "**2: Keep only articles of the same category using text classification.**\n",
        "\n",
        "**3: Extract tags from these articles.**\n",
        "\n",
        "**4: Show the top 5 recommended articles.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29uwe-jzJ9rh",
        "outputId": "ad5294ce-1428-4ce2-b25f-b3fda6648f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cohere\n",
            "  Downloading cohere-2.4.2.tar.gz (9.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from cohere) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->cohere) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->cohere) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->cohere) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->cohere) (2.10)\n",
            "Building wheels for collected packages: cohere\n",
            "  Building wheel for cohere (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cohere: filename=cohere-2.4.2-cp37-cp37m-linux_x86_64.whl size=10617 sha256=62d800e7d16beebc3f659a8e52b3df6b1060857416905de9b6cfe6f758605bd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/cc/f8/b54b1de6d920f2f8cdbf1ea1f161abdaccb88fdd122ae45b4c\n",
            "Successfully built cohere\n",
            "Installing collected packages: cohere\n",
            "Successfully installed cohere-2.4.2\n"
          ]
        }
      ],
      "source": [
        "! pip install cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y9-RyLu7KHII",
        "outputId": "fa5aec7c-8fad-4e8b-d097-9df604106eac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "CohereError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCohereError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cohere/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, version, num_workers, request_dict, check_api_key)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mCohereError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid api key'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mCohereError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCohereError\u001b[0m: invalid api key",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mCohereError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ab38b9005b51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcohere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{apikey}'\u001b[0m \u001b[0;31m# Paste your API key here. Remember to not share it publicly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cohere/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, version, num_workers, request_dict, check_api_key)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCohereError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid api key'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mCohereError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mCohereError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_status\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mResponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCohereError\u001b[0m: invalid api key"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import cohere\n",
        "api_key = '{apikey}' # Paste your API key here. Remember to not share it publicly \n",
        "co = cohere.Client(api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeR8yeXl4YtQ"
      },
      "source": [
        "# **1: Find the most similar articles to the one currently reading using embeddings**\n",
        "\n",
        "![Step 1 - Embed](https://github.com/cohere-ai/notebooks/raw/main/notebooks/images/article-recommender/article-rec-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efe2XFpWqsw7"
      },
      "source": [
        "## 1.1: Get articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJMuR_Pb5TUh"
      },
      "source": [
        "Throughout this article, we'll use the [BBC news article dataset](https://www.kaggle.com/competitions/learn-ai-bbc/data?select=BBC+News+Train.csv) as an example [[Source]](http://mlg.ucd.ie/datasets/bbc.html). This dataset consists of articles from a few categories: business, politics, tech, entertainment, and sport.\n",
        "\n",
        "We'll extract a subset of the data and in Step 1, use the first 100 data points.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G95txfh1KHCu"
      },
      "outputs": [],
      "source": [
        "# Load the dataset to a dataframe\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/cohere-ai/notebooks/main/notebooks/data/bbc_news_subset.csv', delimiter=',')\n",
        "\n",
        "# Select a portion of the dataset \n",
        "INP_START = 0\n",
        "INP_END = 100\n",
        "df_inputs = df.iloc[INP_START:INP_END]\n",
        "df_inputs = df_inputs.copy()\n",
        "\n",
        "# Remove columns we don't need\n",
        "df_inputs.drop(['ArticleId','Category'],axis=1,inplace=True)\n",
        "\n",
        "# View the data\n",
        "df_inputs.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_kIeQpENJzp"
      },
      "source": [
        "## 1.2: Turn articles into embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu3fRvs3A5oe"
      },
      "source": [
        "Next we turn each article text into embeddings. An [embedding](https://docs.cohere.ai/embedding-wiki) is a list of numbers that our models use to represent a piece of text, capturing its context and meaning.\n",
        "\n",
        "We do this by calling Cohere's [Embed endpoint](https://docs.cohere.ai/embed-reference), which takes in text as input and returns embeddings as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoLcyJLsKHAN"
      },
      "outputs": [],
      "source": [
        "# Get text embeddings via the Embed endpoint\n",
        "articles = df_inputs['Text'].tolist()\n",
        "\n",
        "output = co.embed(\n",
        "            model ='large',\n",
        "            texts = articles)\n",
        "embeds = output.embeddings\n",
        "\n",
        "print('Number of articles:', len(embeds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqOIWI_LM2Rc"
      },
      "source": [
        "## 1.3: Pick one article and find the most similar articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIKvKKgJBcnK"
      },
      "source": [
        "Next, we pick any one article to be the one the reader is currently reading (let's call this the target) and find other articles with the most similar embeddings (let's call these candidates) using cosine similarity.\n",
        "\n",
        "[Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is a metric that measures how similar sequences of numbers are (embeddings in our case), and we compute it for each target-candidate pair. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfJJZ6Z0NaWg"
      },
      "outputs": [],
      "source": [
        "# Choose one article ID as the one you are currently reading\n",
        "print(f'Choose one article ID between {INP_START} and {INP_END-1} below...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn0lvlBuNwq_"
      },
      "outputs": [],
      "source": [
        "# Enter your article ID\n",
        "READING_IDX = 70\n",
        "\n",
        "# Get embedding for the article\n",
        "reading = embeds[READING_IDX]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhhaiIsEQF9k"
      },
      "outputs": [],
      "source": [
        "# Calculate cosine similarity between the target and candidate articles\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def get_similarity(target,candidates):\n",
        "  # Turn list into array\n",
        "  candidates = np.array(candidates)\n",
        "  target = np.expand_dims(np.array(target),axis=0)\n",
        "\n",
        "  # Calculate cosine similarity\n",
        "  similarity_scores = cosine_similarity(target,candidates)\n",
        "  similarity_scores = np.squeeze(similarity_scores).tolist()\n",
        "\n",
        "  # Sort by descending order in similarity\n",
        "  similarity_scores = list(enumerate(similarity_scores))\n",
        "  similarity_scores = sorted(similarity_scores, key=lambda x:x[1], reverse=True)\n",
        "\n",
        "  # Return similarity scores\n",
        "  return similarity_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVNQ0cUDR-5U"
      },
      "outputs": [],
      "source": [
        "# Get the similarity between the target and candidate articles\n",
        "similarity = get_similarity(reading,embeds)\n",
        "\n",
        "# View the top 5 articles\n",
        "print('Target:')\n",
        "print(f'[ID {READING_IDX}]',df_inputs['Text'][READING_IDX][:100],'...','\\n')\n",
        "\n",
        "print('Candidates:')\n",
        "for i in similarity[1:6]: # Exclude the target article\n",
        "  print(f'[ID {i[0]}]',df_inputs['Text'][i[0]][:100],'...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpeETEhfyvOa"
      },
      "source": [
        "# **2: Keep only articles of the same category using text classification**\n",
        "\n",
        "![Step 2 - Classify](https://github.com/cohere-ai/notebooks/raw/main/notebooks/images/article-recommender/article-rec-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-neV3KEyyQP"
      },
      "source": [
        "In the example above (Article ID 70 as the target), we see that the top 5 most similar articles given by the system are very relevant. The target is a football/soccer article, and the system duly recommended very similar articles despite this dataset also containing articles from other sports like tennis and rugby.\n",
        " \n",
        "However, not all of them are. The fourth recommended article (ID 86) is not a sports article, but rather politics. Reading the text, it's likely because the target is news about a clash of individuals (i.e. anger about a racism fine), which happens to be what that politics article is also about (i.e. disagreement over an apology). So these two articles' meanings are similar in this way, captured in the embeddings.\n",
        " \n",
        "Perhaps we can make the system better by only recommending articles of the same category. For this, let's build a news category classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viuvclW_fqdf"
      },
      "source": [
        "## 2.1: Build a classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GFksI1A8mHS"
      },
      "source": [
        "We use Cohere's [Classify endpoint](https://docs.cohere.ai/classify-reference) to build a news category classifier, classifying articles into five categories: Business, Politics, Tech, Entertainment, and Sport. \n",
        "\n",
        "A typical text classification model requires hundreds/thousands of data points to train, but with this endpoint, we can build a classifier with a few as five examples per class.\n",
        "\n",
        "To build the classifier, we need a set of examples consisting of text (news text) and labels (news category). The BBC News dataset happens to have both (columns 'Text' and 'Category'), so this time weâ€™ll use the categories for building our examples. For this, we will set aside another portion of dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBJ5SQ5-zrJr"
      },
      "outputs": [],
      "source": [
        "# Select a portion of the dataset to sample the classification examples from\n",
        "EX_START = 100\n",
        "EX_END = 200\n",
        "df_examples = df.iloc[EX_START:EX_END]\n",
        "df_examples = df_examples.copy()\n",
        "\n",
        "# Remove columns we don't need\n",
        "df_examples.drop(['ArticleId'],axis=1,inplace=True)\n",
        "\n",
        "# View the data\n",
        "df_examples.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e2_yP0t-ciE"
      },
      "source": [
        "With the Classify endpoint, there is a limit of 2048 tokens with the medium model. This means full articles won't be able to fit in the examples, so we will approximate and limit each article to its first 300 characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7l_B_87-dfT"
      },
      "outputs": [],
      "source": [
        "# Shorten the example articles (because the medium endpoint max token limit is 2048)\n",
        "MAX_CHARS = 300\n",
        "\n",
        "def shorten_text(text):\n",
        "  return text[:MAX_CHARS]\n",
        "\n",
        "df_examples['Text'] = df_examples['Text'].apply(shorten_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkqfMw019fvH"
      },
      "source": [
        "The Classify endpoint needs a minimum of 5 examples for each category, which we will sample randomly from the dataset. We have 5 categories, so we will have a total of 25 examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUdZ6y3g-ZEN"
      },
      "outputs": [],
      "source": [
        "# Set the number of examples per category\n",
        "EX_PER_CAT = 5 \n",
        "\n",
        "# Get the list of all available categories\n",
        "categories = df_examples['Category'].unique().tolist()\n",
        "\n",
        "# Create list of examples containing texts and labels\n",
        "ex_texts = []\n",
        "ex_labels = []\n",
        "for category in categories:\n",
        "  df_category = df_examples[df_examples['Category'] == category]\n",
        "  samples = df_category.sample(n=EX_PER_CAT, random_state=42)\n",
        "  ex_texts += samples['Text'].tolist()\n",
        "  ex_labels += samples['Category'].tolist()\n",
        "\n",
        "print(f'Number of examples per category: {EX_PER_CAT}')\n",
        "print(f'List of categories: {categories}')\n",
        "print(f'Number of categories: {len(categories)}')\n",
        "print(f'Total number of examples: {len(ex_texts)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuFzl0IS--SA"
      },
      "source": [
        "Once the examples are ready, we can now get the classifications. Here is a function that returns the classification given an input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNFPu99N_Agc"
      },
      "outputs": [],
      "source": [
        "# Get classifications via the Classify endpoint\n",
        "\n",
        "from cohere.classify import Example\n",
        "\n",
        "# Collate the examples\n",
        "examples = []\n",
        "for txt, lbl in zip(ex_texts,ex_labels):\n",
        "  examples.append(Example(txt,lbl))\n",
        "\n",
        "# Perform classification\n",
        "def classify_text(text,examples):\n",
        "  classifications = co.classify(\n",
        "    model='medium',\n",
        "    taskDescription='',\n",
        "    outputIndicator='',\n",
        "    inputs=[text],\n",
        "    examples=examples\n",
        "    )\n",
        "  return classifications.classifications[0].prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ3HJ8Ep4uYk"
      },
      "source": [
        "## 2.2: Measure its performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spu8JHGX_C68"
      },
      "source": [
        "Before actually using the classifier, let's first test its performance. Here we take another 100 data points as the test dataset and the classifier will predict its class i.e. news category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OwI7MzZ0Ev0"
      },
      "outputs": [],
      "source": [
        "# Select a portion of the dataset for testing the classifier\n",
        "TEST_START = 200\n",
        "TEST_END = 300\n",
        "df_test = df.iloc[TEST_START:TEST_END]\n",
        "df_test = df_test.copy()\n",
        "\n",
        "# Remove columns we don't need\n",
        "df_test.drop(['ArticleId'],axis=1,inplace=True)\n",
        "\n",
        "# Shorten the text to fit token limit\n",
        "df_test['Text'] = df_test['Text'].apply(shorten_text)\n",
        "\n",
        "# View the data\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kkt3E5_c_O5c"
      },
      "outputs": [],
      "source": [
        "# Run classifier predictions on the test dataset (this will take a few minutes)\n",
        "\n",
        "# Predicted classes\n",
        "# predictions = []\n",
        "# for article in df_test['Text']:\n",
        "#   predictions.append(classify_text(article,examples))\n",
        "\n",
        "predictions = df_test['Text'].apply(classify_text, args=(examples,)).tolist()\n",
        "\n",
        "# Actual classes\n",
        "actual = df_test['Category'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ10tnuB_QKf"
      },
      "outputs": [],
      "source": [
        "# Compute metrics on the test dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(actual, predictions)\n",
        "print(f'Accuracy: {accuracy*100}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Rqzz1w_SiB"
      },
      "source": [
        "We get a good accuracy score of 91%, so the classifier is ready to be \n",
        "implemented in our recommender system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaY7t_1DJBiy"
      },
      "source": [
        "# **3: Extract tags from these articles.**\n",
        "\n",
        "![Step 3 - Extract](https://github.com/cohere-ai/notebooks/raw/main/notebooks/images/article-recommender/article-rec-4.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8hoJCh7Tv01"
      },
      "source": [
        "We now proceed to the tags extraction step. Compared to the previous two steps, this step is not about sorting or filtering articles, but rather enriching them with more information. \n",
        "\n",
        "We do this by [prompting](https://docs.cohere.ai/prompt-engineering-wiki) Cohere's [Generate endpoint](https://docs.cohere.ai/generate-reference) with a few examples of text and its tags. We then feed the articles from the classifier step and the endpoint will generate the corresponding tags.\n",
        "\n",
        "There are more than one way to construct the prompt, depending on what you'd lile to extract. In my case, the tags I'd like to extract are primarily the names of a person, company, or organization, and perhaps also some generic keywords. That was the idea behind the example tags I put in the prompt, which you can see on the Cohere Playground screenshot below...\n",
        "\n",
        "![Tag extraction prompt](https://github.com/cohere-ai/notebooks/raw/main/notebooks/images/article-recommender/article-rec-6.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIhfHWO0JBRd"
      },
      "outputs": [],
      "source": [
        "# Create the base prompt containing extraction examples\n",
        "# base_prompt = 'Given a news article, please return the list tags containing proper nouns.\\n\\nArticle: japanese banking battle at an end japan s sumitomo mitsui financial has withdrawn its takeover offer for rival bank ufj holdings  enabling the latter to merge with mitsubishi tokyo.  sumitomo bosses told counterparts at ufj of its decision on friday  clearing the way for it to conclude a 3 trillion\\n\\nTags: sumitomo mitsui financial, ufj holdings, mitsubishi tokyo\\n--\\nArticle: france starts digital terrestrial france has become the last big european country to launch a digital terrestrial tv (dtt) service.  initially  more than a third of the population will be able to receive 14 free-to-air channels. despite the long wait for a french dtt roll-out  the new platform s bac\\n\\nTags: france\\n--\\nArticle: apple laptop is  greatest gadget  the apple powerbook 100 has been chosen as the greatest gadget of all time  by us magazine mobile pc.  the 1991 laptop was chosen because it was one of the first  lightweight  portable computers and helped define the layout of all future notebook pcs. the magazine h\\n\\nTags: apple, apple powerbook 100, mobile pc\\n--\\nArticle:'\n",
        "\n",
        "base_prompt = 'Given a news article, please return the list tags containing keywords of that article.\\n\\nArticle: japanese banking battle at an end japan s sumitomo mitsui financial has withdrawn its takeover offer for rival bank ufj holdings  enabling the latter to merge with mitsubishi tokyo.  sumitomo bosses told counterparts at ufj of its decision on friday  clearing the way for it to conclude a 3 trillion\\n\\nTags: sumitomo mitsui financial, ufj holdings, mitsubishi tokyo, japanese banking\\n--\\nArticle: france starts digital terrestrial france has become the last big european country to launch a digital terrestrial tv (dtt) service.  initially  more than a third of the population will be able to receive 14 free-to-air channels. despite the long wait for a french dtt roll-out  the new platform s bac\\n\\nTags: france, digital terrestrial\\n--\\nArticle: apple laptop is  greatest gadget  the apple powerbook 100 has been chosen as the greatest gadget of all time  by us magazine mobile pc.  the 1991 laptop was chosen because it was one of the first  lightweight  portable computers and helped define the layout of all future notebook pcs. the magazine h\\n\\nTags: apple, apple powerbook 100, laptop\\n--\\nArticle:'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqg40uJbXs9T"
      },
      "source": [
        "We call the endpoint by specifying a few settings, and it will generate the corresponding extractions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6j0t4VjJJBNp"
      },
      "outputs": [],
      "source": [
        "# Get extractions via the Generate endpoint\n",
        "def extract_tags(complete_prompt):\n",
        "  prediction = co.generate(\n",
        "    model='xlarge',\n",
        "    prompt=complete_prompt,\n",
        "    max_tokens=30,\n",
        "    temperature=0.3,\n",
        "    k=0,\n",
        "    p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "    stop_sequences=[\"--\"],\n",
        "    return_likelihoods='NONE')\n",
        "  return prediction.generations[0].text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzcXdkhR6hVy"
      },
      "source": [
        "# **4: Show the top 5 recommended articles.**\n",
        "\n",
        "![Complete all steps](https://github.com/cohere-ai/notebooks/raw/main/notebooks/images/article-recommender/article-rec-5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnoP7DcLLiLf"
      },
      "source": [
        "*Note: The outputs may vary. This notebook uses the following model versions: Embed (large-20220425), Classify (medium-20220425), and Generate (xlarge-20220427)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLyUW3Gp4KRB"
      },
      "source": [
        "Let's now put everything together for our article recommender system.\n",
        "\n",
        "First, we select the target article and compute the similarity scores against the candidate articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVTSzK6AXoS3"
      },
      "outputs": [],
      "source": [
        "# Choose one article ID as the one you are currently reading\n",
        "print(f'Choose one article ID between {INP_START} and {INP_END-1} below...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM7mBSGvJBKd"
      },
      "outputs": [],
      "source": [
        "# Enter your article ID\n",
        "READING_IDX = 70\n",
        "\n",
        "# Get embedding for the article\n",
        "reading = embeds[READING_IDX]\n",
        "\n",
        "# Get the similarity between the target and candidate articles\n",
        "similarity = get_similarity(reading,embeds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECFlqp2k5jET"
      },
      "source": [
        "Next, we filter the articles via classification. Finally, we extract the keywords from each article and show the recommendations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eviOxPUZJBGo"
      },
      "outputs": [],
      "source": [
        "# Choose the number of articles to recommend\n",
        "SHOW_TOP = 5\n",
        "\n",
        "# Shorten the text to fit token limit\n",
        "df_inputs = df_inputs.copy()\n",
        "df_inputs['Text'] = df_inputs['Text'].apply(shorten_text)\n",
        "\n",
        "# Get the recommendations\n",
        "def get_recommendations(reading_idx,similarity,show_top):\n",
        "\n",
        "  # Show the current article\n",
        "  print('------  You are reading...  ------')\n",
        "  print(f'[ID {READING_IDX}] Article:',df_inputs['Text'][reading_idx][:MAX_CHARS]+'...\\n')\n",
        "\n",
        "  # Show the recommended articles\n",
        "  print('------  You might also like...  ------')\n",
        "\n",
        "  # Classify the target article\n",
        "  target_class = classify_text(df_inputs['Text'][reading_idx],examples) \n",
        "\n",
        "  count = 0\n",
        "  for idx,score in similarity:\n",
        "\n",
        "    # Classify each candidate article\n",
        "    candidate_class = classify_text(df_inputs['Text'][idx],examples)\n",
        "\n",
        "    # Show recommendations\n",
        "    if target_class == candidate_class and idx != reading_idx:\n",
        "      selection = df_inputs['Text'][idx][:MAX_CHARS]\n",
        "      print(f'[ID {idx}] Article:',selection+'...')\n",
        "\n",
        "      # Extract and show tags\n",
        "      tags_raw = extract_tags(base_prompt+selection)\n",
        "      tags_clean = re.findall(\"(?s)Tags: (.+?)\\\\n\",tags_raw)\n",
        "      if tags_clean:\n",
        "          print(f'Tags: {str(tags_clean[0])}\\n')\n",
        "      else:\n",
        "          print(f'Tags: none\\n')      \n",
        "\n",
        "      # Increment the article count\n",
        "      count += 1\n",
        "\n",
        "      # Stop once articles reach the SHOW_TOP number\n",
        "      if count == show_top:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im9BmZYJ614y"
      },
      "outputs": [],
      "source": [
        "# Show the recommended articles\n",
        "get_recommendations(READING_IDX,similarity,SHOW_TOP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVhljS_4ILM2"
      },
      "source": [
        "Keeping to the Section 1.3 example, here we see how the classification and extraction steps have improved our recommendation outcome.\n",
        "\n",
        "First, now the politics article (ID 86) doesn't get recommended anymore. And now we have the tags related to each article being generated. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Mkm7c7_X5D"
      },
      "source": [
        "Let's try a couple of other articles in business and tech and see the output..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNQpkgXvkNtI"
      },
      "source": [
        "Business article (returning recommendations around German economy and economic growth/slump):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utbxE-k7_U1c"
      },
      "outputs": [],
      "source": [
        "# A business news article example (ID 40)\n",
        "\n",
        "# Enter your article ID\n",
        "READING_IDX = 1\n",
        "\n",
        "# Get embedding for the article\n",
        "reading = embeds[READING_IDX]\n",
        "\n",
        "# Get the similarity between the target and candidate articles\n",
        "similarity = get_similarity(reading,embeds)\n",
        "\n",
        "# Show the recommended articles\n",
        "get_recommendations(READING_IDX,similarity,SHOW_TOP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYitPtmJkUo5"
      },
      "source": [
        "Tech article (returning recommendations around consumer devices):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGpZxk0j_6G4"
      },
      "outputs": [],
      "source": [
        "# A tech news article example (ID 30)\n",
        "\n",
        "# Enter your article ID\n",
        "READING_IDX = 71\n",
        "\n",
        "# Get embedding for the article\n",
        "reading = embeds[READING_IDX]\n",
        "\n",
        "# Get the similarity between the target and candidate articles\n",
        "similarity = get_similarity(reading,embeds)\n",
        "\n",
        "# Show the recommended articles\n",
        "get_recommendations(READING_IDX,similarity,SHOW_TOP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f55W2I9RZzXz"
      },
      "source": [
        "In conclusion, this demonstrates an example of how we can stack multiple NLP endpoints together to get an output much closer to our desired outcome.\n",
        "\n",
        "In practice, hosting and maintaining multiple models can turn quickly into a complex activity. But by leveraging Cohere endpoints, this task is reduced to a simple API call."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Article Recommender with Text Embedding, Classification, and Extraction.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit ('3.10.0')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "1fb8019e3560b882083e525615cf48e713d3a7345a15eb723d805e91aa410aac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}